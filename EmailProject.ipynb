{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "torch.cuda.is_available()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_new_email(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    parts = text.split('\"')  \n",
    "    \n",
    "    subject = parts[1].strip()\n",
    "    body = parts[3].strip()\n",
    "    label = parts[5].strip()\n",
    "    \n",
    "    new_email = {'Subject':subject, 'Body':body, 'Label':label}\n",
    "    return new_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "confirmation_emails = []\n",
    "\n",
    "for i in range(1, 93):\n",
    "    file_path = f\"Data/confirmation/conf{i}.txt\"\n",
    "    new_email = read_new_email(file_path)\n",
    "    confirmation_emails.append(new_email)\n",
    "\n",
    "print(len(confirmation_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "rejection_emails = []\n",
    "\n",
    "for i in range(1, 105):\n",
    "    file_path = f\"Data/rejections/rejection{i}.txt\"\n",
    "    new_email = read_new_email(file_path)\n",
    "    rejection_emails.append(new_email)\n",
    "\n",
    "print(len(rejection_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thanks for your application to spotify! dear applicant, we just got your application for the 2024 summer internship, engineering (new york city) role! even though this is just an automated confirmation email, you should know that we’re truly excited you want to join the band. we’ll get back to you as soon as we can. we get a huge amount of applications, and we look at them all to give everyone fair consideration – so it may take a few weeks (or sometimes months for really popular roles). in the meantime, you can listen to the playlist we’ll play in the office as we read your resume. want to sneak a backstage peek? follow life at spotify on linkedin, instagram, twitter, and youtube. learn more about our culture through our band manifesto, or listen to the greenroom and spoton! podcasts. still curious? check out our hr blog, podcasting website, and engineering blog! all the best, the spotify recruiting team'"
      ]
     },
     "execution_count": 4224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0, len(confirmation_emails)):\n",
    "    subject = confirmation_emails[i][\"Subject\"]\n",
    "    body = confirmation_emails[i][\"Body\"]\n",
    "    confirmation_emails[i] = f\"{subject} {body}\"\n",
    "\n",
    "confirmation_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"update on your skydio application - wireless software intern hi applicant, thank you for applying for the wireless software intern role at skydio! we know that there are lots of exciting companies out there, so we appreciate the time you took to apply to ours. after reviewing your application, we have unfortunately decided not to move forward at this time. while we greatly appreciate your interest in skydio, we ultimately decided to proceed with other candidates whose skills and experience align more closely with our needs for this specific role. if you submitted an application to other roles at skydio and haven't heard back, then your resume is still under review and we will be in touch soon with an update. our hiring priorities are constantly evolving, so we encourage you to keep an eye on our jobs page. if you see another role that seems like a good fit, please don't hesitate to apply. in the meantime, we'd like to thank you again for your interest and we wish you all the best in your job search. cheers, skydio talent team\""
      ]
     },
     "execution_count": 4225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(0, len(rejection_emails)):\n",
    "    subject = rejection_emails[i][\"Subject\"]\n",
    "    body = rejection_emails[i][\"Body\"]\n",
    "    rejection_emails[i] = f\"{subject} {body}\"\n",
    "\n",
    "rejection_emails[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\harri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4227,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(confirmation_emails)):\n",
    "    new_text = preprocess_text(confirmation_emails[i])\n",
    "    confirmation_emails[i] = new_text\n",
    "\n",
    "for i in range(0, len(rejection_emails)):\n",
    "    new_text = preprocess_text(rejection_emails[i])\n",
    "    rejection_emails[i] = new_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4228,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmation_labels = [1] * len(confirmation_emails)\n",
    "rejection_labels = [0] * len(rejection_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4229,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = confirmation_emails + rejection_emails\n",
    "labels = confirmation_labels + rejection_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=120)  \n",
    "X = vectorizer.fit_transform(emails).toarray()\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensors = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensors = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensors = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test_tensors = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Function\n",
    "\n",
    "def acc_fn(y_true, y_pred):\n",
    "    y_pred = torch.round(y_pred)\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    accuracy = (correct/len(y_pred)) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmailModel(\n",
       "  (fc1): Linear(in_features=338, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc5): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       "  (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (layer_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (layer_norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (layer_norm4): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 4233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# class EmailModel(nn.Module):\n",
    "#     def __init__(self, input_size):\n",
    "#         super(EmailModel, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, 512)\n",
    "#         self.fc2 = nn.Linear(512, 128)\n",
    "#         self.fc3 = nn.Linear(128, 64)\n",
    "#         self.fc4 = nn.Linear(64, 1)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.relu(self.fc3(x))\n",
    "    \n",
    "\n",
    "#         x = self.fc4(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "# input_size = 338\n",
    "# model_1 = EmailModel(input_size).to(device)\n",
    "# model_1\n",
    "\n",
    "class EmailModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(EmailModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.layer_norm1 = nn.LayerNorm(512)\n",
    "        self.layer_norm2 = nn.LayerNorm(256)\n",
    "        self.layer_norm3 = nn.LayerNorm(128)\n",
    "        self.layer_norm4 = nn.LayerNorm(64)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.layer_norm1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.layer_norm2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.layer_norm3(self.fc3(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.layer_norm4(self.fc4(x)))\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "input_size = 338\n",
    "model_1 = EmailModel(input_size).to(device)\n",
    "model_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Train Loss: 0.7072, Train Acc: 54.49% | Test Loss: 0.6606, Test Acc: 50.00%\n",
      "Epoch: 2 | Train Loss: 0.6656, Train Acc: 55.77% | Test Loss: 1.0291, Test Acc: 52.50%\n",
      "Epoch: 3 | Train Loss: 0.9462, Train Acc: 53.21% | Test Loss: 0.5491, Test Acc: 72.50%\n",
      "Epoch: 4 | Train Loss: 0.5198, Train Acc: 82.05% | Test Loss: 0.7949, Test Acc: 47.50%\n",
      "Epoch: 5 | Train Loss: 0.7442, Train Acc: 48.72% | Test Loss: 0.4036, Test Acc: 95.00%\n",
      "Epoch: 6 | Train Loss: 0.3769, Train Acc: 96.79% | Test Loss: 0.4858, Test Acc: 80.00%\n",
      "Epoch: 7 | Train Loss: 0.4112, Train Acc: 85.90% | Test Loss: 0.4032, Test Acc: 85.00%\n",
      "Epoch: 8 | Train Loss: 0.3166, Train Acc: 90.38% | Test Loss: 0.2412, Test Acc: 95.00%\n",
      "Epoch: 9 | Train Loss: 0.2113, Train Acc: 96.79% | Test Loss: 0.2193, Test Acc: 92.50%\n",
      "Epoch: 10 | Train Loss: 0.1973, Train Acc: 96.79% | Test Loss: 0.3314, Test Acc: 90.00%\n",
      "Epoch: 11 | Train Loss: 0.2132, Train Acc: 93.59% | Test Loss: 0.1411, Test Acc: 97.50%\n",
      "Epoch: 12 | Train Loss: 0.1313, Train Acc: 98.08% | Test Loss: 0.2146, Test Acc: 92.50%\n",
      "Epoch: 13 | Train Loss: 0.1199, Train Acc: 98.08% | Test Loss: 0.3211, Test Acc: 90.00%\n",
      "Epoch: 14 | Train Loss: 0.1277, Train Acc: 97.44% | Test Loss: 0.3845, Test Acc: 87.50%\n",
      "Epoch: 15 | Train Loss: 0.1586, Train Acc: 96.15% | Test Loss: 0.3979, Test Acc: 87.50%\n",
      "Epoch: 16 | Train Loss: 0.1399, Train Acc: 96.15% | Test Loss: 0.4066, Test Acc: 87.50%\n",
      "Epoch: 17 | Train Loss: 0.1152, Train Acc: 97.44% | Test Loss: 0.4135, Test Acc: 87.50%\n",
      "Epoch: 18 | Train Loss: 0.1144, Train Acc: 97.44% | Test Loss: 0.3570, Test Acc: 90.00%\n",
      "Epoch: 19 | Train Loss: 0.1120, Train Acc: 97.44% | Test Loss: 0.2728, Test Acc: 92.50%\n",
      "Epoch: 20 | Train Loss: 0.1089, Train Acc: 97.44% | Test Loss: 0.1828, Test Acc: 92.50%\n",
      "Epoch: 21 | Train Loss: 0.0888, Train Acc: 98.08% | Test Loss: 0.1075, Test Acc: 97.50%\n",
      "Epoch: 22 | Train Loss: 0.0881, Train Acc: 98.08% | Test Loss: 0.1074, Test Acc: 97.50%\n",
      "Epoch: 23 | Train Loss: 0.0880, Train Acc: 98.08% | Test Loss: 0.1073, Test Acc: 97.50%\n",
      "Epoch: 24 | Train Loss: 0.0948, Train Acc: 97.44% | Test Loss: 0.1073, Test Acc: 97.50%\n",
      "Epoch: 25 | Train Loss: 0.0857, Train Acc: 98.08% | Test Loss: 0.1073, Test Acc: 97.50%\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Define the model\n",
    "input_size = X_train_tensors.shape[1]\n",
    "model = EmailModel(input_size).to(device)\n",
    "X_test_tensors = X_test_tensors.to(device)\n",
    "X_train_tensors = X_train_tensors.to(device)\n",
    "y_train_tensors = y_train_tensors.to(device)\n",
    "y_test_tensors = y_test_tensors.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 25\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass\n",
    "    train_preds = model(X_train_tensors).squeeze()\n",
    "    train_loss = loss_fn(train_preds, y_train_tensors)\n",
    "    train_acc = acc_fn(y_train_tensors, train_preds)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        test_preds = model(X_test_tensors).squeeze()\n",
    "        test_loss = loss_fn(test_preds, y_test_tensors)\n",
    "        test_acc = acc_fn(y_test_tensors, test_preds)\n",
    "        \n",
    "    print(f\"Epoch: {epoch+1} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
